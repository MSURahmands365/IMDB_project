{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run this in the terminal and choose notebook kernal .venv\n",
        "# Inorder to check FastApi \n",
        "to run FastAPI server ` uv run python -m uvicorn apiformodel.app.server:app --reload` \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "8Wc_SAPE_Gng"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "KoXvo75H_tGm",
        "outputId": "59ec4acf-9da9-47b0-dfdd-f10a1667fd58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = r'./IMDB Dataset.csv' \n",
        "df = pd.read_csv(path)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mapping The Categorical Column into Numerical "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "ha5S4neSCYXV",
        "outputId": "790fce5b-773c-41b9-d43b-760021f60da5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\msurahman\\AppData\\Local\\Temp\\ipykernel_20116\\2857565349.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['sentiment']=df['sentiment'].replace({\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from numpy import positive\n",
        "df['sentiment']=df['sentiment'].replace({\n",
        "    'positive':1,\n",
        "    'negative':0\n",
        "    })\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "jYJ-lSGkEMk5",
        "outputId": "786db393-12eb-4fc0-bef7-6a3dd5fe0c38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "review       object\n",
              "sentiment     int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "acDBTV8RERkj"
      },
      "outputs": [],
      "source": [
        "x=df['review']\n",
        "y=df['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nd5-E6LEkIr",
        "outputId": "ca8e6572-1cf4-470f-eb0c-9b8e52808026"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(50000)"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "29vXJN4MEnS_",
        "outputId": "74a45642-68c6-45b8-bd6b-641bc3a0561c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "1    25000\n",
              "0    25000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "272LLSq6Et4W",
        "outputId": "fd215e5b-dca1-4e89-9b88-fe595d219a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'those', 'was', \"you'll\", \"we're\", 'were', 'all', 'my', 'ourselves', 'with', \"she'd\", 'about', 'then', 'theirs', 'his', \"we've\", \"you'd\", 'her', 'over', 'do', 'our', 'here', 'too', 'and', \"i've\", \"they'll\", 'only', \"it'd\", 'is', 'because', 'below', 'before', \"she'll\", 'yourself', 'been', \"should've\", 'itself', 'where', 'being', 'so', 'few', 'they', 'what', 'o', \"they've\", \"that'll\", 'such', 'why', 'an', 'd', \"he'd\", 'herself', 'them', 'that', 'by', 'who', 'or', 'just', 'y', 'have', 'your', 'further', 'other', 'again', 'when', 'which', 'into', 'down', \"it'll\", 'you', 'she', 'each', 'once', 'through', 'has', 'on', 'at', \"they're\", 'as', 'its', \"they'd\", 're', 'yours', 'having', 'between', 'should', 'myself', 'some', 'these', 'under', 've', 'whom', 's', 'hers', 'now', \"she's\", \"i'm\", 'am', 'in', 'i', 'any', 'it', 'out', \"you're\", 'there', 'did', 'a', 'this', 'how', 'yourselves', 'be', 'if', 'ours', 'he', 'while', \"you've\", 'the', 'does', 'but', \"i'd\", \"it's\", 'their', 'm', 'to', 'same', 'until', 't', 'me', 'we', 'will', 'than', \"he'll\", \"he's\", 'own', 'after', 'of', 'himself', 'themselves', 'for', 'from', 'both', \"we'd\", 'above', 'him', 'during', \"we'll\", 'ma', 'up', 'll', \"i'll\", 'had', 'can', 'are', 'doing'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\msurahman\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from revised_stopwords import get_revised_stopwords\n",
        "stopwords_list = get_revised_stopwords()\n",
        "nltk.download('punkt_tab')\n",
        "print(stopwords_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip these fields as These are only to see the working of each function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SRkM1zfxHFai",
        "outputId": "1f2eabb0-8bf7-4efa-afd2-85b23ac46941"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'd',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'was',\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words=set(get_revised_stopwords())\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AK70N-HH167",
        "outputId": "efd46561-b404-4384-81c3-4345d45abf8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this', 'is', 'a', 'simple', 'text', 'to', 'remove', 'html', 'tags']"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokens = word_tokenize(text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUKhQDfAIKsa",
        "outputId": "e72cbff6-930f-4954-9ce7-36e948d9c72c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['simple', 'text', 'remove', 'html', 'tags']"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_word=set(stopwords.words('english'))\n",
        "filtered_tokens=[word for word in tokens if word.lower() not in stop_word]\n",
        "filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXBRrn_iRf74",
        "outputId": "7efd8ae9-a10e-4529-b8bb-d45511653196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['simpl', 'text', 'remov', 'html', 'tag']\n",
            "simpl text remov html tag\n"
          ]
        }
      ],
      "source": [
        "stemmed_tokens=[PorterStemmer().stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(stemmed_tokens)\n",
        "preprocessed_text=' '.join(stemmed_tokens)\n",
        "print(preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "gK7p1JeFUc0D"
      },
      "outputs": [],
      "source": [
        "def preprocessed_text(text):\n",
        "  text=re.sub(r'<[^>]+>',' ',text)\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  tokens=word_tokenize(text)\n",
        "  filtered_tokens=[token for token in tokens if token.lower() not in stop_words]\n",
        "  stemmed_tokens=[PorterStemmer().stem(token) for token in filtered_tokens]\n",
        "  preprocessed_text= ' '.join(stemmed_tokens)\n",
        "  return preprocessed_text.lower()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "yCmbeO4cdfqu",
        "outputId": "7f42ba99-e37f-46ef-9cba-dcf15882a2c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one review mention watch 1 oz episod 'll hook ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonder littl product . film techniqu unassumin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonder way spend time hot summer weeke...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basic 's famili littl boy ( jake ) think 's zo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  one review mention watch 1 oz episod 'll hook ...          1\n",
              "1  wonder littl product . film techniqu unassumin...          1\n",
              "2  thought wonder way spend time hot summer weeke...          1\n",
              "3  basic 's famili littl boy ( jake ) think 's zo...          0"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x=[preprocessed_text(text) for text in x]\n",
        "preprocessed_df=pd.DataFrame({\"review\":x,\"sentiment\":y})\n",
        "preprocessed_df.head(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "onFWtj3ff67o"
      },
      "outputs": [],
      "source": [
        "path=r'./cleaned_data.csv'\n",
        "preprocessed_df.to_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpq8gCLTmgtg",
        "outputId": "f001370e-dd03-45cf-c33a-ed06c54d16e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (6.33.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: pillow in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\msurahman\\documents\\projects\\imdb\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spliting Data Set Into Train Test Evaluate\n",
        "# Applying Tokenization To convert Categorical (Reviews) to Numerical Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY26izyjh1s7",
        "outputId": "3e9e9042-f08a-4a8b-f3e7-6975e8008613"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  448,  662,   82],\n",
              "       [   0,    0,    0, ...,  159, 6898, 6459],\n",
              "       [   0,    0,    0, ...,  296,  893,    2],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 2444,  314,  210],\n",
              "       [   0,    0,    0, ...,   45,  554,  184],\n",
              "       [   0,    0,    0, ...,  319,  606,  707]],\n",
              "      shape=(2500, 500), dtype=int32)"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_texts,test_texts,train_labels,test_labels=train_test_split(x,y,test_size=0.05,random_state=42)\n",
        "num_words=10000\n",
        "maxlen=500\n",
        "tokenizer=Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "\n",
        "train_sequences=tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences=tokenizer.texts_to_sequences(test_texts)\n",
        "#we have to give a fixed length of words to an algo that is why we are applying this\n",
        "train_data=pad_sequences(train_sequences,maxlen=maxlen)\n",
        "test_data=pad_sequences(test_sequences,maxlen=maxlen)\n",
        "\n",
        "test_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training \n",
        "(take 36 minutes to train on 10000 top words )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6y-p60Ap2Ak",
        "outputId": "d1b9f4e1-232a-48f2-95ba-5b2412bd93f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\msurahman\\Documents\\projects\\IMDB\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 1s/step - accuracy: 0.8423 - loss: 0.3509 - val_accuracy: 0.8924 - val_loss: 0.2641\n",
            "Epoch 2/5\n",
            "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.9261 - loss: 0.1972 - val_accuracy: 0.8860 - val_loss: 0.2769\n",
            "Epoch 3/5\n",
            "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 1s/step - accuracy: 0.9514 - loss: 0.1351 - val_accuracy: 0.8887 - val_loss: 0.2925\n",
            "Epoch 4/5\n",
            "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 2s/step - accuracy: 0.9699 - loss: 0.0901 - val_accuracy: 0.8765 - val_loss: 0.3523\n",
            "Epoch 5/5\n",
            "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 1s/step - accuracy: 0.9855 - loss: 0.0480 - val_accuracy: 0.8806 - val_loss: 0.4451\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.8832 - loss: 0.3745\n",
            "Test accuracy: 0.8832\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout,Bidirectional\n",
        "\n",
        "# 1.  Model Definition\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Bidirectional(LSTM(64)),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 2. Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 3. Train\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    batch_size=128,\n",
        "                    epochs=5,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# 4. Evaluate\n",
        "loss, accuracy = model.evaluate(test_data, test_labels)\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Model and Tokenizer to use later For Prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCRbUeD_sYc8",
        "outputId": "ed26d210-e61c-47fe-9956-c3eaa2e006de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and Tokenizer saved to our Project Directory\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "save_model_path=r'./models/imdb_sentiment_model.keras'\n",
        "model.save(save_model_path)\n",
        "save_tokenizer_path=r'./models/tokenizer.pickle'\n",
        "with open(save_tokenizer_path, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Model and Tokenizer saved to our Project Directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference by Utilizing the same Model and Tokenizer \n",
        "can start from here only to use model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNUaFxByyA9W"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[32m      5\u001b[39m MODEL_PATH = \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./models/imdb_sentiment_model.keras\u001b[39m\u001b[33m'\u001b[39m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "MODEL_PATH = r'./models/imdb_sentiment_model.keras'\n",
        "model_for_prediction = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "\n",
        "with open(r'./models/tokenizer.pickle', 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n",
        "\n",
        "def quick_predict(raw_text):\n",
        "   \n",
        "    cleaned = clean_text(raw_text)\n",
        "    \n",
        "    sequence = loaded_tokenizer.texts_to_sequences([cleaned])\n",
        "    \n",
        "    if not sequence[0]:\n",
        "        print(\"Warning: Tokenizer did not recognize any words in this sentence!\")\n",
        "    \n",
        "    padded = pad_sequences(sequence, maxlen=500)\n",
        "    prediction = model_for_prediction.predict(padded, verbose=0)[0][0]\n",
        "\n",
        "    if prediction >= 0.5:\n",
        "        print(f\"Result: POSITIVE ({prediction*100:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"Result: NEGATIVE ({(1-prediction)*100:.2f}%)\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing the input For inference "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "vFogguTqyTwb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from revised_stopwords import get_revised_stopwords\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower().split()\n",
        "    ps = PorterStemmer()\n",
        "    stop_words = set(get_revised_stopwords())\n",
        "    text = [ps.stem(word) for word in text if word not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "cleaned_input = clean_text(\"I hated every minute of it. A total waste of time.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtHJYrdGz5sv",
        "outputId": "9ca1b04a-ddc5-4758-f162-b3d3be891085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: NEGATIVE (99.97%)\n"
          ]
        }
      ],
      "source": [
        "quick_predict(cleaned_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Taking Input From User For Inference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS73q8yAzxZz",
        "outputId": "82cdf722-078d-47c7-e6b9-8dcbb1381607"
      },
      "source": [
        "user_review = input(\"Please write your movie review: \")\n",
        "\n",
        "if len(user_review) > 100:\n",
        "    user_review = user_review[:100]\n",
        "\n",
        "print(f\"Processing text: '{user_review}'\")\n",
        "clean_txt= clean_text(user_review)\n",
        "print(f\"Processed text: '{clean_txt}'\")\n",
        "quick_predict(clean_txt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing text: 'this movie does not amaze me'\n",
            "clean text: 'movi not amaz'\n",
            "Result: NEGATIVE (65.27%)\n"
          ]
        }
      ],
      "source": [
        "user_review = input(\"Please write your movie review: \")\n",
        "\n",
        "if len(user_review) > 100:\n",
        "    user_review = user_review[:100]\n",
        "\n",
        "print(f\"Processing text: '{user_review}'\")\n",
        "clean_txt= clean_text(user_review)\n",
        "quick_predict(clean_txt)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "imdb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
